{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Amazon SageMaker models for molecular property prediction by using DGL with PyTorch backend\n",
    "\n",
    "The **Amazon SageMaker Python SDK** makes it easy to train Deep Graph Library (DGL) models. In this example, you train a simple graph neural network for molecular toxicity prediction by using [DGL](https://github.com/dmlc/dgl) and the Tox21 dataset.\n",
    "\n",
    "The dataset contains qualitative toxicity measurements for 8,014 compounds on 12 different targets, including nuclear \n",
    "receptors and stress-response pathways. Each target yields a binary classification problem. You can model the problem as a graph classification problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a few variables that you need later in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# Setup session\n",
    "sess = sagemaker.Session()\n",
    "# in case we need to use a different region than the one in our AWS CLI config:\n",
    "# sess = sagemaker.Session(boto3.session.Session(region_name='eu-west-1'))\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "# Location to put your custom code.\n",
    "custom_code_upload_location = \"customcode\"\n",
    "\n",
    "# IAM execution role that gives Amazon SageMaker access to resources in your AWS account.\n",
    "# You can use the Amazon SageMaker Python SDK to get the role from the notebook environment.\n",
    "#role = get_execution_role()\n",
    "role = 'arn:aws:iam::819888036505:role/service-role/AmazonSageMaker-ExecutionRole-20211101T165507'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main.py` provides all the code you need for training a molecular property prediction model by using Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\n",
      "import json\n",
      "import os\n",
      "import random\n",
      "from datetime import datetime\n",
      "\n",
      "import dgl\n",
      "import numpy as np\n",
      "import torch\n",
      "from dgl import model_zoo\n",
      "from dgl.data.chem import Tox21\n",
      "from dgl.data.utils import split_dataset\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from torch.nn import BCEWithLogitsLoss\n",
      "from torch.optim import Adam\n",
      "from torch.utils.data import DataLoader\n",
      "\n",
      "\n",
      "def setup(args, seed=0):\n",
      "    args[\"device\"] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "\n",
      "    # Set random seed\n",
      "    random.seed(seed)\n",
      "    np.random.seed(seed)\n",
      "    torch.manual_seed(seed)\n",
      "    if torch.cuda.is_available():\n",
      "        torch.cuda.manual_seed(seed)\n",
      "    return args\n",
      "\n",
      "\n",
      "def collate_molgraphs(data):\n",
      "    \"\"\"Batching a list of datapoints for dataloader.\"\"\"\n",
      "    smiles, graphs, labels, masks = map(list, zip(*data))\n",
      "\n",
      "    bg = dgl.batch(graphs)\n",
      "    bg.set_n_initializer(dgl.init.zero_initializer)\n",
      "    bg.set_e_initializer(dgl.init.zero_initializer)\n",
      "    labels = torch.stack(labels, dim=0)\n",
      "    masks = torch.stack(masks, dim=0)\n",
      "    return smiles, bg, labels, masks\n",
      "\n",
      "\n",
      "class EarlyStopper(object):\n",
      "    def __init__(self, patience, filename=None):\n",
      "        if filename is None:\n",
      "            # Name checkpoint based on time\n",
      "            dt = datetime.now()\n",
      "            filename = \"early_stop_{}_{:02d}-{:02d}-{:02d}.pth\".format(\n",
      "                dt.date(), dt.hour, dt.minute, dt.second\n",
      "            )\n",
      "            filename = os.path.join(\"/opt/ml/model\", filename)\n",
      "\n",
      "        self.patience = patience\n",
      "        self.counter = 0\n",
      "        self.filename = filename\n",
      "        self.best_score = None\n",
      "        self.early_stop = False\n",
      "\n",
      "    def save_checkpoint(self, model):\n",
      "        \"\"\"Saves model when the metric on the validation set gets improved.\"\"\"\n",
      "        torch.save({\"model_state_dict\": model.state_dict()}, self.filename)\n",
      "\n",
      "    def load_checkpoint(self, model):\n",
      "        \"\"\"Load model saved with early stopping.\"\"\"\n",
      "        model.load_state_dict(torch.load(self.filename)[\"model_state_dict\"])\n",
      "\n",
      "    def step(self, score, model):\n",
      "        if (self.best_score is None) or (score > self.best_score):\n",
      "            self.best_score = score\n",
      "            self.save_checkpoint(model)\n",
      "            self.counter = 0\n",
      "        else:\n",
      "            self.counter += 1\n",
      "            print(\"EarlyStopping counter: {:d} out of {:d}\".format(self.counter, self.patience))\n",
      "            if self.counter >= self.patience:\n",
      "                self.early_stop = True\n",
      "        return self.early_stop\n",
      "\n",
      "\n",
      "class Meter(object):\n",
      "    \"\"\"Track and summarize model performance on a dataset for\n",
      "    (multi-label) binary classification.\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.mask = []\n",
      "        self.y_pred = []\n",
      "        self.y_true = []\n",
      "\n",
      "    def update(self, y_pred, y_true, mask):\n",
      "        \"\"\"Update for the result of an iteration\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        y_pred : float32 tensor\n",
      "            Predicted molecule labels with shape (B, T),\n",
      "            B for batch size and T for the number of tasks\n",
      "        y_true : float32 tensor\n",
      "            Ground truth molecule labels with shape (B, T)\n",
      "        mask : float32 tensor\n",
      "            Mask for indicating the existence of ground\n",
      "            truth labels with shape (B, T)\n",
      "        \"\"\"\n",
      "        self.y_pred.append(y_pred.detach().cpu())\n",
      "        self.y_true.append(y_true.detach().cpu())\n",
      "        self.mask.append(mask.detach().cpu())\n",
      "\n",
      "    def roc_auc_score(self):\n",
      "        \"\"\"Compute roc-auc score for each task.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        list of float\n",
      "            roc-auc score for all tasks\n",
      "        \"\"\"\n",
      "        mask = torch.cat(self.mask, dim=0)\n",
      "        y_pred = torch.cat(self.y_pred, dim=0)\n",
      "        y_true = torch.cat(self.y_true, dim=0)\n",
      "        # This assumes binary case only\n",
      "        y_pred = torch.sigmoid(y_pred)\n",
      "        n_tasks = y_true.shape[1]\n",
      "        scores = []\n",
      "        for task in range(n_tasks):\n",
      "            task_w = mask[:, task]\n",
      "            task_y_true = y_true[:, task][task_w != 0].numpy()\n",
      "            task_y_pred = y_pred[:, task][task_w != 0].numpy()\n",
      "            scores.append(roc_auc_score(task_y_true, task_y_pred))\n",
      "        return scores\n",
      "\n",
      "\n",
      "def run_a_train_epoch(args, epoch, model, data_loader, loss_criterion, optimizer):\n",
      "    model.train()\n",
      "    train_meter = Meter()\n",
      "    for batch_id, batch_data in enumerate(data_loader):\n",
      "        smiles, bg, labels, masks = batch_data\n",
      "        atom_feats = bg.ndata.pop(args[\"atom_data_field\"])\n",
      "        atom_feats, labels, masks = (\n",
      "            atom_feats.to(args[\"device\"]),\n",
      "            labels.to(args[\"device\"]),\n",
      "            masks.to(args[\"device\"]),\n",
      "        )\n",
      "        logits = model(bg, atom_feats)\n",
      "        # Mask non-existing labels\n",
      "        loss = (loss_criterion(logits, labels) * (masks != 0).float()).mean()\n",
      "        optimizer.zero_grad()\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        print(\n",
      "            \"epoch {:d}/{:d}, batch {:d}/{:d}, loss {:.4f}\".format(\n",
      "                epoch + 1, args[\"n_epochs\"], batch_id + 1, len(data_loader), loss.item()\n",
      "            )\n",
      "        )\n",
      "        train_meter.update(logits, labels, masks)\n",
      "    train_score = np.mean(train_meter.roc_auc_score())\n",
      "    print(\n",
      "        \"epoch {:d}/{:d}, training roc-auc {:.4f}\".format(epoch + 1, args[\"n_epochs\"], train_score)\n",
      "    )\n",
      "\n",
      "\n",
      "def run_an_eval_epoch(args, model, data_loader):\n",
      "    model.eval()\n",
      "    eval_meter = Meter()\n",
      "    with torch.no_grad():\n",
      "        for batch_id, batch_data in enumerate(data_loader):\n",
      "            smiles, bg, labels, masks = batch_data\n",
      "            atom_feats = bg.ndata.pop(args[\"atom_data_field\"])\n",
      "            atom_feats, labels = atom_feats.to(args[\"device\"]), labels.to(args[\"device\"])\n",
      "            logits = model(bg, atom_feats)\n",
      "            eval_meter.update(logits, labels, masks)\n",
      "    return np.mean(eval_meter.roc_auc_score())\n",
      "\n",
      "\n",
      "def load_sagemaker_config(args):\n",
      "    file_path = \"/opt/ml/input/config/hyperparameters.json\"\n",
      "    if os.path.isfile(file_path):\n",
      "        with open(file_path, \"r\") as f:\n",
      "            new_args = json.load(f)\n",
      "            for k, v in new_args.items():\n",
      "                if k not in args:\n",
      "                    continue\n",
      "                if isinstance(args[k], int):\n",
      "                    v = int(v)\n",
      "                if isinstance(args[k], float):\n",
      "                    v = float(v)\n",
      "                args[k] = v\n",
      "    return args\n",
      "\n",
      "\n",
      "def main(args):\n",
      "    args = setup(args)\n",
      "\n",
      "    dataset = Tox21()\n",
      "    train_set, val_set, test_set = split_dataset(dataset, shuffle=True)\n",
      "    train_loader = DataLoader(\n",
      "        train_set, batch_size=args[\"batch_size\"], shuffle=True, collate_fn=collate_molgraphs\n",
      "    )\n",
      "    val_loader = DataLoader(\n",
      "        val_set, batch_size=args[\"batch_size\"], shuffle=True, collate_fn=collate_molgraphs\n",
      "    )\n",
      "    test_loader = DataLoader(\n",
      "        test_set, batch_size=args[\"batch_size\"], shuffle=True, collate_fn=collate_molgraphs\n",
      "    )\n",
      "\n",
      "    model = model_zoo.chem.GCNClassifier(\n",
      "        in_feats=args[\"n_input\"],\n",
      "        gcn_hidden_feats=[args[\"n_hidden\"] for _ in range(args[\"n_layers\"])],\n",
      "        n_tasks=dataset.n_tasks,\n",
      "        classifier_hidden_feats=args[\"n_hidden\"],\n",
      "    ).to(args[\"device\"])\n",
      "    loss_criterion = BCEWithLogitsLoss(\n",
      "        pos_weight=torch.tensor(dataset.task_pos_weights).to(args[\"device\"]), reduction=\"none\"\n",
      "    )\n",
      "    optimizer = Adam(model.parameters(), lr=args[\"lr\"])\n",
      "    stopper = EarlyStopper(args[\"patience\"])\n",
      "\n",
      "    for epoch in range(args[\"n_epochs\"]):\n",
      "        # Train\n",
      "        run_a_train_epoch(args, epoch, model, train_loader, loss_criterion, optimizer)\n",
      "\n",
      "        # Validation and early stop\n",
      "        val_score = run_an_eval_epoch(args, model, val_loader)\n",
      "        early_stop = stopper.step(val_score, model)\n",
      "        print(\n",
      "            \"epoch {:d}/{:d}, validation roc-auc {:.4f}, best validation roc-auc {:.4f}\".format(\n",
      "                epoch + 1, args[\"n_epochs\"], val_score, stopper.best_score\n",
      "            )\n",
      "        )\n",
      "        if early_stop:\n",
      "            break\n",
      "\n",
      "    stopper.load_checkpoint(model)\n",
      "    test_score = run_an_eval_epoch(args, model, test_loader)\n",
      "    print(\"Best validation score {:.4f}\".format(stopper.best_score))\n",
      "    print(\"Test score {:.4f}\".format(test_score))\n",
      "\n",
      "\n",
      "def parse_args():\n",
      "    parser = argparse.ArgumentParser(description=\"GCN for Tox21\")\n",
      "    parser.add_argument(\n",
      "        \"--batch-size\", type=int, default=128, help=\"Number of graphs (molecules) per batch\"\n",
      "    )\n",
      "    parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate\")\n",
      "    parser.add_argument(\n",
      "        \"--n-epochs\", type=int, default=100, help=\"Maximum number of training epochs\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--atom-data-field\", type=str, default=\"h\", help=\"Name for storing atom features\"\n",
      "    )\n",
      "    parser.add_argument(\"--n-input\", type=int, default=74, help=\"Size for input atom features\")\n",
      "    parser.add_argument(\"--n-hidden\", type=int, default=64, help=\"Size for hidden representations\")\n",
      "    parser.add_argument(\"--n-layers\", type=int, default=2, help=\"Number of hidden layers\")\n",
      "    parser.add_argument(\n",
      "        \"--patience\", type=int, default=10, help=\"Number of epochs to wait before early stop\"\n",
      "    )\n",
      "    return parser.parse_args().__dict__\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    args = parse_args()\n",
      "    args = load_sagemaker_config(args)\n",
      "    main(args)\n"
     ]
    }
   ],
   "source": [
    "!cat main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring Your Own Image for Amazon SageMaker\n",
    "\n",
    "In this example, you need rdkit library to handle the tox21 dataset. The DGL CPU and GPU Docker has the rdkit library pre-installed at Dockerhub under dgllib registry (namely, dgllib/dgl-sagemaker-cpu:dgl_0.4_pytorch_1.2.0_rdkit for CPU and dgllib/dgl-sagemaker-gpu:dgl_0.4_pytorch_1.2.0_rdkit for GPU). You can pull the image yourself according to your requirement and push it into your AWS ECR. Following script helps you to do so. You can skip this step if you have already prepared your DGL Docker image in your Amazon Elastic Container Registry (Amazon ECR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgl_0.4_pytorch_1.2.0_rdkit: Pulling from dgllib/dgl-sagemaker-gpu\n",
      "Digest: sha256:5e99e0336dbd4ffab576d11f5dfbbcf020dffb7d22bd8204f9a56a0ec9e9a103\n",
      "Status: Image is up to date for dgllib/dgl-sagemaker-gpu:dgl_0.4_pytorch_1.2.0_rdkit\n",
      "docker.io/dgllib/dgl-sagemaker-gpu:dgl_0.4_pytorch_1.2.0_rdkit\n",
      "819888036505\n",
      "Login Succeeded\n",
      "The push refers to repository [819888036505.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-dgl-pytorch-gcn-tox21]\n",
      "503ea5ce1b0d: Preparing\n",
      "5f5519b1e2be: Preparing\n",
      "68cde6786b69: Preparing\n",
      "bd4c9ad79e39: Preparing\n",
      "e918970a6388: Preparing\n",
      "3b183c3d9548: Preparing\n",
      "9e174541fd90: Preparing\n",
      "e18671bb6f71: Preparing\n",
      "25f6fb6fff6f: Preparing\n",
      "5bde1457d341: Preparing\n",
      "dfe12520986d: Preparing\n",
      "ff4d40f8732b: Preparing\n",
      "41ceebac7737: Preparing\n",
      "f398437e4634: Preparing\n",
      "b42b4fab3e2e: Preparing\n",
      "8464e4a1821e: Preparing\n",
      "ed88571bd95c: Preparing\n",
      "69d90c18d3c5: Preparing\n",
      "76993a8d1a18: Preparing\n",
      "2eafd5e86d56: Preparing\n",
      "1673fa18caaf: Preparing\n",
      "7545d8b4edec: Preparing\n",
      "718bbdc0b45f: Preparing\n",
      "4a78de7ea906: Preparing\n",
      "0bfa7a55184c: Preparing\n",
      "122be11ab4a2: Preparing\n",
      "7beb13bce073: Preparing\n",
      "f7eae43028b3: Preparing\n",
      "6cebf3abed5f: Preparing\n",
      "3b183c3d9548: Waiting\n",
      "9e174541fd90: Waiting\n",
      "e18671bb6f71: Waiting\n",
      "25f6fb6fff6f: Waiting\n",
      "5bde1457d341: Waiting\n",
      "dfe12520986d: Waiting\n",
      "ff4d40f8732b: Waiting\n",
      "41ceebac7737: Waiting\n",
      "f398437e4634: Waiting\n",
      "b42b4fab3e2e: Waiting\n",
      "8464e4a1821e: Waiting\n",
      "ed88571bd95c: Waiting\n",
      "69d90c18d3c5: Waiting\n",
      "76993a8d1a18: Waiting\n",
      "2eafd5e86d56: Waiting\n",
      "1673fa18caaf: Waiting\n",
      "7545d8b4edec: Waiting\n",
      "718bbdc0b45f: Waiting\n",
      "4a78de7ea906: Waiting\n",
      "0bfa7a55184c: Waiting\n",
      "122be11ab4a2: Waiting\n",
      "7beb13bce073: Waiting\n",
      "f7eae43028b3: Waiting\n",
      "6cebf3abed5f: Waiting\n",
      "503ea5ce1b0d: Layer already exists\n",
      "bd4c9ad79e39: Layer already exists\n",
      "5f5519b1e2be: Layer already exists\n",
      "e918970a6388: Layer already exists\n",
      "68cde6786b69: Layer already exists\n",
      "3b183c3d9548: Layer already exists\n",
      "9e174541fd90: Layer already exists\n",
      "e18671bb6f71: Layer already exists\n",
      "5bde1457d341: Layer already exists\n",
      "25f6fb6fff6f: Layer already exists\n",
      "ff4d40f8732b: Layer already exists\n",
      "dfe12520986d: Layer already exists\n",
      "b42b4fab3e2e: Layer already exists\n",
      "41ceebac7737: Layer already exists\n",
      "f398437e4634: Layer already exists\n",
      "ed88571bd95c: Layer already exists\n",
      "8464e4a1821e: Layer already exists\n",
      "69d90c18d3c5: Layer already exists\n",
      "2eafd5e86d56: Layer already exists\n",
      "76993a8d1a18: Layer already exists\n",
      "1673fa18caaf: Layer already exists\n",
      "7545d8b4edec: Layer already exists\n",
      "718bbdc0b45f: Layer already exists\n",
      "4a78de7ea906: Layer already exists\n",
      "0bfa7a55184c: Layer already exists\n",
      "122be11ab4a2: Layer already exists\n",
      "7beb13bce073: Layer already exists\n",
      "f7eae43028b3: Layer already exists\n",
      "6cebf3abed5f: Layer already exists\n",
      "latest: digest: sha256:ee58cbd4d9a26b84e5f404cf29456d668427b0a947abea3342b6b756de612cad size: 6417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#1 [internal] load build definition from gcn_tox21_gpu.Dockerfile\n",
      "#1 sha256:b8233447d862e61f63780f1cc64a43508dc1eb33736bb6bfa806a2e612930dfb\n",
      "#1 transferring dockerfile: 483B 0.0s done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load .dockerignore\n",
      "#2 sha256:a97985ac6fd5e9b7592f3ddd56e1b222e171081cb0e0a20eb69a263158502b7d\n",
      "#2 transferring context: 2B done\n",
      "#2 DONE 0.0s\n",
      "\n",
      "#3 [internal] load metadata for docker.io/dgllib/dgl-sagemaker-gpu:dgl_0.4_pytorch_1.2.0_rdkit\n",
      "#3 sha256:8e0eded9f7afcc575efc5a4ec5255bb3fceae061ef4ee18bdda86de169fbd0d3\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [1/3] FROM docker.io/dgllib/dgl-sagemaker-gpu:dgl_0.4_pytorch_1.2.0_rdkit\n",
      "#4 sha256:27174b8b886c9788772c2d407aa529eec3fc19c5ac2439d1f783d3aaa8dd7cea\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [2/3] RUN pip install -U scikit-learn\n",
      "#5 sha256:96b7ffb4b58b2cc2cec97abe59888849936269185d968c1b93c5e00fff4c92f4\n",
      "#5 CACHED\n",
      "\n",
      "#6 [3/3] RUN pip install pandas\n",
      "#6 sha256:41c01a89c71eeb3a83a8bf104cc131f44fc0bb7a7e2c0744a5dc525de9fb50bf\n",
      "#6 CACHED\n",
      "\n",
      "#7 exporting to image\n",
      "#7 sha256:e8c613e07b0b7ff33893b694f7759a10d42e180f2b4dc349fb57dc6b71dcab00\n",
      "#7 exporting layers done\n",
      "#7 writing image sha256:8b221577fcd49786ffca18bb6c5d965fce812a5151aeb38dff585f75865ed2c3 done\n",
      "#7 naming to docker.io/library/sagemaker-dgl-pytorch-gcn-tox21 done\n",
      "#7 DONE 0.0s\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'sagemaker-dgl-pytorch-gcn-tox21' already exists in the registry with id '819888036505'\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# For CPU default_docker_name=\"dgllib/dgl-sagemaker-cpu:dgl_0.4_pytorch_1.2.0_rdkit\"\n",
    "default_docker_name=\"dgllib/dgl-sagemaker-gpu:dgl_0.4_pytorch_1.2.0_rdkit\"\n",
    "docker pull $default_docker_name\n",
    "\n",
    "docker_name=sagemaker-dgl-pytorch-gcn-tox21\n",
    "\n",
    "# For CPU docker build -t $docker_name -f gcn_tox21_cpu.Dockerfile .\n",
    "docker build -t $docker_name -f gcn_tox21_gpu.Dockerfile .\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "echo $account\n",
    "region=$(aws configure get region)\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${docker_name}:latest\"\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${docker_name}\" > /dev/null 2>&1\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${docker_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "docker tag ${docker_name} ${fullname}\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Amazon SageMaker Estimator class\n",
    "\n",
    "The Amazon SageMaker Estimator allows you to run a single machine in Amazon SageMaker, using CPU or GPU-based instances.\n",
    "\n",
    "When you create the estimator, pass in the file name of the training script and the name of the IAM execution role. Also provide a few other parameters. `train_instance_count` and `train_instance_type` determine the number and type of SageMaker instances that will be used for the training job. The hyperparameters can be passed to the training script via a dict of values. See `main.py` for how they are handled.\n",
    "\n",
    "The entrypoint of Amazon SageMaker Docker (e.g., dgllib/dgl-sagemaker-gpu:dgl_0.4_pytorch_1.2.0_rdkit) is a train script under /usr/bin/. The train script inside dgl docker image provided above will try to get the real entrypoint from the hyperparameters (with the key 'entrypoint') and run the real entrypoint under 'training-code' data channel (/opt/ml/input/data/training-code/) .\n",
    "\n",
    "For this example, choose one ml.p3.2xlarge instance. You can also use a CPU instance such as ml.c4.2xlarge for the CPU image. You can also add a task_tag with value 'DGL' to help tracking the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819888036505.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-dgl-pytorch-gcn-tox21:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Set target dgl-docker name\n",
    "docker_name = \"sagemaker-dgl-pytorch-gcn-tox21\"\n",
    "\n",
    "CODE_PATH = \"main.py\"\n",
    "code_location = sess.upload_data(CODE_PATH, bucket=bucket, key_prefix=custom_code_upload_location)\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name\n",
    "image = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, docker_name)\n",
    "print(image)\n",
    "task_tags = [{\"Key\": \"ML Task\", \"Value\": \"DGL\"}]\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.p3.2xlarge\",  #'ml.c4.2xlarge'\n",
    "    #train_instance_type=\"ml.g2.2xlarge\",\n",
    "    hyperparameters={\"entrypoint\": CODE_PATH},\n",
    "    tags=task_tags,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Training Job\n",
    "\n",
    "After you construct an Estimator object, fit it by using Amazon SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-02 16:59:18 Starting - Starting the training job...\n",
      "2021-11-02 16:59:41 Starting - Launching requested ML instancesProfilerReport-1635872358: InProgress\n",
      "...\n",
      "2021-11-02 17:00:14 Starting - Preparing the instances for training............\n",
      "2021-11-02 17:02:22 Downloading - Downloading input data\n",
      "2021-11-02 17:02:22 Training - Downloading the training image................\u001b[34m/opt/ml/input/data/training-code /\u001b[0m\n",
      "\u001b[34mDownloading /root/.dgl/tox21.csv.gz from https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/tox21.csv.gz...\u001b[0m\n",
      "\u001b[34mdownload failed, retrying, 4 attempts left\u001b[0m\n",
      "\u001b[34mDownloading /root/.dgl/tox21.csv.gz from https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/tox21.csv.gz...\u001b[0m\n",
      "\u001b[34mdownload failed, retrying, 3 attempts left\u001b[0m\n",
      "\u001b[34mDownloading /root/.dgl/tox21.csv.gz from https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/tox21.csv.gz...\u001b[0m\n",
      "\u001b[34mdownload failed, retrying, 2 attempts left\u001b[0m\n",
      "\u001b[34mDownloading /root/.dgl/tox21.csv.gz from https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/tox21.csv.gz...\u001b[0m\n",
      "\u001b[34mdownload failed, retrying, 1 attempt left\u001b[0m\n",
      "\u001b[34mDownloading /root/.dgl/tox21.csv.gz from https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/tox21.csv.gz...\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"main.py\", line 260, in <module>\n",
      "    main(args)\n",
      "  File \"main.py\", line 191, in main\n",
      "    dataset = Tox21()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/dgl/data/chem/tox21.py\", line 43, in __init__\n",
      "    download(_get_dgl_url(self._url), path=data_path)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/dgl/data/utils.py\", line 161, in download\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/dgl/data/utils.py\", line 147, in download\n",
      "    raise RuntimeError(\"Failed downloading url %s\" % url)\u001b[0m\n",
      "\u001b[34mRuntimeError: Failed downloading url https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/tox21.csv.gz\u001b[0m\n",
      "\u001b[34m/\u001b[0m\n",
      "\n",
      "2021-11-02 17:05:23 Uploading - Uploading generated training model\n",
      "2021-11-02 17:05:23 Completed - Training job completed\n",
      "Training seconds: 194\n",
      "Billable seconds: 194\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training-code\": code_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output\n",
    "You can get the model training output from the Amazon Sagemaker console by searching for the training task and looking for the address of 'S3 model artifact'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b2fbfef80eb32c5bdf9ea8ba2a12dc59c07859ad931117d426a72bc22eafbf6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('sagemaker_notebooks': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
